# Indian-Sign-Language
 The ** Indian Sign Language to Text/Speech** Translation project aims to bridge this gap by developing a system that translates ISL gestures into text and speech, using advanced techniques in Deep Learning, Computer Vision, and Natural Language Processing (NLP). This solution is designed to facilitate seamless communication, fostering inclusivity in educational, professional, and social settings.
The 2011 Indian census cites roughly 1.3 million people with “hearing impairment”. In contrast to that numbers from India’s National Association of the Deaf estimates that 18 million people –roughly 1 percent of the Indian population are deaf. These statistics formed the motivation for our project. As these speech impairments and deaf people need a proper channel to communicate with normal people there is a need for a system. Not all normal people can understand the sign language of impaired people. Our project hence is aimed at converting the sign language gestures into text that is readable for normal people .
It is a language that includes gestures made with the hands and other body parts, including facial expressions and postures of the body. It is used primarily by people who are deaf and dumb. There are many different sign languages as, British, Indian and American sign languages. British sign language (BSL) is not easily intelligible to users of American Sign Language (ASL) and vice versa. A functioning signing recognition system could provide a chance for the inattentive to communicate with non-signing people without the necessity for an interpreter. It might be wont to generate speech or text making the deaf more independent. Unfortunately, there has not been any system with these capabilities thus far. during this project, we aim to develop a system that may classify signing accurately. American Sign Language (ASL) is a complete, natural language that has the same linguistic properties as spoken languages, with grammar that differs from English. ASL is expressed by movements of the hands and face.  
The overall system consists of five modular components:
1.Data Ingestion & Labeling : Uses Python’s pathlib and glob to traverse a directory tree of per‐gesture subfolders, collecting image paths and extracting category labels into a Pandas DataFrame for downstream processing.
2.Data Preprocessing & Augmentation : Applies grayscale conversion, normalization (rescaling to [0,1]), and optional traditional feature extraction (thresholding, edge detection, skeletonization). On-the-fly augmentation—random zoom, shear, and brightness adjustments—via Keras’s ImageDataGenerator enhances robustness.
3.Model Training : Defines a sequential CNN with four convolutional blocks (Conv2D→ReLU→BatchNorm→Dropout→MaxPool) followed by fully connected layers. Compiled with Adam optimizer and categorical cross-entropy loss, the model is trained using early stopping and checkpoint callbacks.
4.Evaluation & Analysis : Computes loss and accuracy on a held-out test set. Generates confusion matrices and classification reports to identify common misclassifications and guide future improvements.
5.Real-Time Inference : Captures live video frames via OpenCV, preprocesses each frame to match model input dimensions, predicts the sign class, and overlays the predicted character on the video feed at interactive frame rates (<50 ms per frame).
